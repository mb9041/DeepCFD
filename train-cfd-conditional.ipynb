{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto reload the module\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from datasets import load_dataset\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch \n",
    "from torch.utils.data import TensorDataset\n",
    "from diffusers import UNet2DModel, UNet2DConditionModel\n",
    "from PIL import Image\n",
    "from diffusers import DDPMScheduler\n",
    "import torch.nn.functional as F\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "from diffusers import DDPMPipeline\n",
    "import math\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "from flow_generator import FlowGenerator\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    '''\n",
    "    Class for training parameters\n",
    "    '''\n",
    "    \n",
    "    image_size = 128  # the generated image resolution\n",
    "    train_batch_size = 4\n",
    "    eval_batch_size = 4  # how many images to sample during evaluation\n",
    "    num_epochs = 200\n",
    "    gradient_accumulation_steps = 1\n",
    "    learning_rate = 1e-4\n",
    "    lr_warmup_steps = 500\n",
    "    save_image_epochs = 10\n",
    "    save_model_epochs = 30\n",
    "    mixed_precision = \"fp16\"  # `no` for float32, `fp16` for automatic mixed precision\n",
    "    output_dir = \"/home/maddie/Documents/underwater/DeepCFD/output-conditional\"  # the model name locally and on the HF Hub\n",
    "\n",
    "    push_to_hub = False  # whether to upload the saved model to the HF Hub\n",
    "    hub_private_repo = False\n",
    "    overwrite_output_dir = True  # overwrite the old model when re-running the notebook\n",
    "    seed = 0\n",
    "\n",
    "\n",
    "config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pickle files using the exact file paths\n",
    "c_data_path = \"/home/maddie/Documents/underwater/DeepCFD/dataX.pkl\"\n",
    "y_data_path = \"/home/maddie/Documents/underwater/DeepCFD/dataY.pkl\"\n",
    "\n",
    "# conditional data \n",
    "with open(c_data_path, \"rb\") as f:\n",
    "    c = pickle.load(f)\n",
    "\n",
    "# output data (data we want to predict )\n",
    "with open(y_data_path, \"rb\") as f:\n",
    "    y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot output channels\n",
    "fig, axs = plt.subplots(1, 3, figsize=(25, 5))  # 1 row, 3 columns for the 3 channels\n",
    "\n",
    "for i in range(3):  # Loop over the channels\n",
    "    ax = axs[i]  # Get the current axis\n",
    "    channel_data = y[10, i, :, :] # Get the data for the first sample, channel i\n",
    "    im = ax.imshow(channel_data.T, cmap='jet', aspect='auto')  # Plot the data\n",
    "    ax.set_title(f'Channel {i+1}')  # Set the title for the channel\n",
    "    fig.colorbar(im, ax=ax)  # Add a colorbar for each plot\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot input channels\n",
    "fig, axs = plt.subplots(1, 3, figsize=(25, 5))  # 1 row, 3 columns for the 3 channels\n",
    "\n",
    "# Clip the negative values to 0\n",
    "temp = c[10,0]\n",
    "temp = np.where(temp<=0, 0, temp)\n",
    "c[10,0] = temp\n",
    "\n",
    "for i in range(3):  # Loop over the channels\n",
    "    ax = axs[i]  # Get the current axis\n",
    "    channel_data = c[10, i, :, :] # Get the data for the first sample, channel i\n",
    "    im = ax.imshow(channel_data.T, cmap='jet', aspect='auto')  # Plot the data\n",
    "    ax.set_title(f'Channel {i+1}')  # Set the title for the channel\n",
    "    fig.colorbar(im, ax=ax)  # Add a colorbar for each plot\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize input data\n",
    "# Assuming you want to randomly sample an input data point\n",
    "index = 301\n",
    "\n",
    "# Plotting only the input data (input features)\n",
    "input_data = c[index]\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(input_data.transpose(1, 2, 0), cmap = \"jet\")  # Transpose dimensions for correct display\n",
    "plt.title('Input Data')\n",
    "plt.xlabel('Width')\n",
    "plt.ylabel('Height')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the first 50 inputs in a 5x10 grid subplot\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "for i in range(50):\n",
    "    plt.subplot(5, 10, i + 1)\n",
    "    input_data = c[i]\n",
    "    plt.imshow(input_data.transpose(1, 2, 0))  # Transpose dimensions for correct display\n",
    "    plt.title('Input {}'.format(i+1))\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "\n",
    "# turn the input in a pytorch tensor\n",
    "c = torch.FloatTensor(c)\n",
    "y = torch.FloatTensor(y)\n",
    "\n",
    "# Normalize the data [-1,1] for each channel \n",
    "def normalize_tensor(tensor):\n",
    "    min_val = torch.amin(tensor, dim=(0, 2, 3), keepdim=True)  # min over all axes except channels\n",
    "    max_val = torch.amax(tensor, dim=(0, 2, 3), keepdim=True)  # max over all axes except channels\n",
    "    # Normalize to [-1, 1]\n",
    "    tensor_normalized = 2 * ((tensor - min_val) / (max_val - min_val)) - 1\n",
    "    return tensor_normalized\n",
    "\n",
    "# normalize the input and output data\n",
    "c = normalize_tensor(c)\n",
    "y = normalize_tensor(y)\n",
    "\n",
    "# Plot normalized output channels \n",
    "fig, axs = plt.subplots(1, 3, figsize=(25, 5))  # 1 row, 3 columns for the 3 channels\n",
    "\n",
    "for i in range(3):  # Loop over the channels\n",
    "    ax = axs[i]  # Get the current axis\n",
    "    channel_data = y[10, i, :, :] # Get the data for the first sample, channel i\n",
    "    im = ax.imshow(channel_data.T, cmap='jet', aspect='auto')  # Plot the data\n",
    "    ax.set_title(f'Channel {i+1}')  # Set the title for the channel\n",
    "    fig.colorbar(im, ax=ax)  # Add a colorbar for each plot\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function for dividing the dataset \n",
    "def split_tensors(*tensors, ratio):\n",
    "    preprocess = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((128,128)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            # transforms.ToTensor(),\n",
    "            # transforms.Normalize([0.5], [0.5]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    assert len(tensors) > 0\n",
    "    split1, split2 = [], []\n",
    "    count = len(tensors[0])\n",
    "    for tensor in tensors:\n",
    "        assert len(tensor) == count\n",
    "        tensor = [preprocess(temp) for temp in tensor]\n",
    "        tensor = torch.stack(tensor)\n",
    "        split1.append(tensor[:int(len(tensor) * ratio)])\n",
    "        split2.append(tensor[int(len(tensor) * ratio):])\n",
    "    if len(tensors) == 1:\n",
    "        split1, split2 = split1[0], split2[0]\n",
    "    return split1, split2\n",
    "\n",
    "# Split the data into training and testing sets (70/30)\n",
    "train_data, test_data = split_tensors(c,y,ratio=0.7)\n",
    "\n",
    "# train_data and test_data are lists containing two tensors each: [inputs, outputs]\n",
    "train_dataset = TensorDataset(*train_data)\n",
    "test_dataset = TensorDataset(*test_data)\n",
    "\n",
    "# Print the shapes of the tensors in train_data\n",
    "print(\"Train Data Length: \", len(train_data))\n",
    "train_data_c = train_data[0]\n",
    "train_data_y = train_data[1]\n",
    "print(\"Input training data shape: \", train_data_c.shape)\n",
    "print(\"Output training data shape: \", train_data_y.shape)\n",
    "print()\n",
    "\n",
    "# Print the shapes of the tensors in test_data\n",
    "print(\"Test Data Length: \", len(test_data))\n",
    "test_data_c = test_data[0]\n",
    "test_data_y = test_data[1]\n",
    "print(\"Input test data shape: \", test_data_c.shape)\n",
    "print(\"Output test data shape: \", test_data_y.shape)\n",
    "\n",
    "# Load data into PyTorch DataLoader\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=config.train_batch_size, shuffle=True)\n",
    "\n",
    "test_c, test_y = test_data[:] # split test data into x and y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Encode conditional input data \n",
    "# condidition, flowfield = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import a geometry encoder\n",
    "# from geometry_encoder import GeometryEncoder\n",
    "# geo_encoder = GeometryEncoder(\n",
    "#     in_channels=3,\n",
    "#     down_block_types = (\"DownBlock2D\", \"AttnDownBlock2D\", \"AttnDownBlock2D\", \"AttnDownBlock2D\"),\n",
    "#     block_out_channels = (224, 448, 672, 896), \n",
    "# )\n",
    "# geo_encoder.cuda()\n",
    "# geo_output = geo_encoder(condidition.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Input shape: {condidition.shape}\")\n",
    "# for i, output in enumerate(geo_output):\n",
    "#     print(f\"Output {i+1} shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Scheduler \n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=10000) # DDPM scheduler with 1000 timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.shape)\n",
    "# Visualize the effect of noise on an image\n",
    "def visualize_noisy_image(image_tensor, noise_scheduler, timesteps):\n",
    "    # Generate random noise\n",
    "    noise = torch.randn(image_tensor.shape)\n",
    "\n",
    "    # Add noise using the scheduler\n",
    "    noisy_image = noise_scheduler.add_noise(image_tensor, noise, timesteps)\n",
    "\n",
    "    # Convert the noisy image for visualization\n",
    "    noisy_image_to_display = Image.fromarray(((noisy_image.permute(0, 2, 3, 1).detach() + 1) * 127.5).type(torch.uint8).numpy()[0])\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(noisy_image_to_display)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "channel_data = y[10, i, :, :]\n",
    "# Visualize a noisy version of the first sample in the training data\n",
    "# You can change the timestep to see the effect of different noise levels\n",
    "visualize_noisy_image(image_tensor = y, \n",
    "                      noise_scheduler = noise_scheduler, \n",
    "                      timesteps = torch.LongTensor([50])\n",
    "                      )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize adding noise at different timesteps\n",
    "def visualize_noisy_image(image_tensor, noise_scheduler, timesteps):\n",
    "    fig, axes = plt.subplots(1, len(timesteps), figsize=(20, 4))\n",
    "    \n",
    "    for i, step in enumerate(timesteps):\n",
    "        # Generate random noise\n",
    "        noise = torch.randn_like(image_tensor)\n",
    "        # Add noise using the scheduler\n",
    "        noisy_image = noise_scheduler.add_noise(image_tensor, noise, step)\n",
    "\n",
    "        # Normalize the noisy image for visualization\n",
    "        noisy_image_to_display = ((noisy_image.squeeze().detach() + 1) * 127.5).clamp(0, 255).type(torch.uint8)\n",
    "        noisy_image_to_display = noisy_image_to_display.permute(1, 2, 0).numpy()\n",
    "\n",
    "        # Display the image\n",
    "        axes[i].imshow(noisy_image_to_display)\n",
    "        axes[i].set_title(f'Timestep {step.item()}')\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Visualize a noisy version of the first sample in the training data at different timesteps\n",
    "timesteps = torch.LongTensor([0, 100, 200, 300, 400, 500, 600, 700]) # example timesteps\n",
    "visualize_noisy_image(y[0:1], noise_scheduler, timesteps=timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define unet model for noise prediction \n",
    "# unet_model = UNet2DConditionModel(\n",
    "#     in_channels=3,  # number of input channels\n",
    "#     out_channels=3,  # number of output channels\n",
    "#     down_block_types=(\"CrossAttnDownBlock2D\", \"CrossAttnDownBlock2D\", \"CrossAttnDownBlock2D\", \"DownBlock2D\"),\n",
    "#     mid_block_type=\"UNetMidBlock2DCrossAttn\",\n",
    "#     up_block_types = (\"UpBlock2D\", \"CrossAttnUpBlock2D\", \"CrossAttnUpBlock2D\", \"CrossAttnUpBlock2D\"),\n",
    "#     block_out_channels =  (224, 448, 672, 896),\n",
    "#     addition_embed_type = 'image',\n",
    "#     encoder_hid_dim = 896,\n",
    "# )\n",
    "# unet_model = unet_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FlowGenerator(\n",
    "    in_channels = 3,\n",
    "    out_channels = 3, \n",
    "    geometry_channels = 3,\n",
    "    layers_per_block=2,\n",
    "    down_block_types =(\n",
    "        \"DownBlock2D\",  # a regular ResNet downsampling block\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
    "        \"DownBlock2D\"\n",
    "    ),\n",
    "    mid_block_type = \"UNetMidBlock2DCrossAttn\",\n",
    "    up_block_types = (\n",
    "        \"UpBlock2D\",  # a regular ResNet upsampling block\n",
    "        \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "    ),\n",
    "    geometry_block_types = (\n",
    "        \"DownBlock2D\",  # a regular ResNet downsampling block\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
    "        \"DownBlock2D\"\n",
    "    ),\n",
    "    block_out_channels = (128, 128, 256, 256, 512, 512),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer:  update the model's parameters based on the computed gradients during the training process\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "# Learning rate scheduler: adjust the learning rate of the optimizer during the training process\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=config.lr_warmup_steps,\n",
    "    num_training_steps=(len(train_dataloader) * config.num_epochs),\n",
    ")\n",
    "\n",
    "# Function for evaluation\n",
    "def evaluate(config, epoch, model, noise_scheduler, val_dataloader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    total_mse = 0\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for batch in val_dataloader:\n",
    "            clean_images = batch[1].to(device)\n",
    "            conditions = batch[0].to(device)\n",
    "            noise = torch.randn(clean_images.shape).to(device)\n",
    "            bs = clean_images.shape[0]\n",
    "            timesteps = torch.randint(\n",
    "                0, noise_scheduler.config.num_train_timesteps, (bs,), device=device\n",
    "            ).long()\n",
    "            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "            noise_pred = model(sample = noisy_images,\n",
    "                               condition = conditions,\n",
    "                               time = timesteps) # forward\n",
    "            mse = F.mse_loss(noise_pred, noise, reduction='sum').item()  # Calculate batch MSE\n",
    "            total_mse += mse\n",
    "\n",
    "    avg_mse = total_mse / len(val_dataloader.dataset)\n",
    "    print(f\"Epoch {epoch}: Average MSE = {avg_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Method to evaluate the model\n",
    "\n",
    "# Define the training loop function\n",
    "def train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler):\n",
    "    \n",
    "    # Check if an output directory is specified and create it if it doesn't exist\n",
    "    if config.output_dir is not None:\n",
    "        os.makedirs(config.output_dir, exist_ok=True)\n",
    "        # Initialize any desired loggers here, such as TensorBoard\n",
    "\n",
    "    # Initialize a global step counter\n",
    "    global_step = 0\n",
    "\n",
    "    # GPU/CPU setup\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Start the training process\n",
    "    for epoch in range(config.num_epochs):\n",
    "        \n",
    "        # Initialize a progress bar for the epoch\n",
    "        progress_bar = tqdm(total=len(train_dataloader), desc=f\"Epoch {epoch}\")\n",
    "        \n",
    "        # Iterate over the training data loader\n",
    "        for _, batch in enumerate(train_dataloader):\n",
    "\n",
    "            # Move the images from the current batch to the device\n",
    "            clean_images = batch[1].to(device)\n",
    "\n",
    "            conditions = batch[0].to(device)\n",
    "\n",
    "            # Generate random noise of the same shape as the images\n",
    "            noise = torch.randn(clean_images.shape).to(device)\n",
    "\n",
    "            # Get the batch size from the images shape\n",
    "            bs = clean_images.shape[0]\n",
    "\n",
    "            # Sample random timesteps for each image in the batch\n",
    "            timesteps = torch.randint(\n",
    "                0, noise_scheduler.config.num_train_timesteps, (bs,), device=device\n",
    "            ).long()\n",
    "\n",
    "            # Add noise to the clean images according to the noise magnitude at the sampled timesteps\n",
    "            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "\n",
    "            # print(\"noisy data:\", noisy_images.shape)\n",
    "            # print(\"timesteps:\", timesteps.shape)\n",
    "\n",
    "            # Predict the noise residual using the model\n",
    "            noise_pred = model(sample = noisy_images,\n",
    "                               condition = conditions,\n",
    "                               time = timesteps) # forward\n",
    "            # print(\"noise_pred:\", noise_pred.shape)\n",
    "            \n",
    "            # Calculate the mean squared error loss between the predicted noise and the actual noise\n",
    "            loss = F.mse_loss(noise_pred, noise)\n",
    "\n",
    "            # Perform backpropagation: zero the gradients, calculate gradients, and perform a single optimization step\n",
    "            optimizer.zero_grad()  # Clear previous gradients\n",
    "            loss.backward()  # Calculate gradients through backpropagation\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Clip gradients to avoid exploding gradient problem\n",
    "            optimizer.step()  # Update model parameters\n",
    "            lr_scheduler.step()  # Update learning rate scheduler\n",
    "\n",
    "            # Update the progress bar and log the loss and learning rate\n",
    "            progress_bar.update(1)\n",
    "            logs = {\"loss\": loss.item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            # Log your progress here, replacing 'accelerator.log' with your logger\n",
    "            global_step += 1  # Increment the global step counter\n",
    "\n",
    "        # After each epoch, evaluate your model and save it if necessary\n",
    "        if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "            # Replace 'evaluate' with your evaluation function\n",
    "            evaluate(config, epoch, model, noise_scheduler, train_dataloader)\n",
    "\n",
    "        if (epoch + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "            # Save your model parameters to the specified output directory\n",
    "            torch.save(model.state_dict(), os.path.join(config.output_dir, f\"model_epoch_{epoch}.pth\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model \n",
    "train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cfd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
