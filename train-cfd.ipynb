{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from datasets import load_dataset\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch \n",
    "from torch.utils.data import TensorDataset\n",
    "from diffusers import UNet2DModel\n",
    "from PIL import Image\n",
    "from diffusers import DDPMScheduler\n",
    "import torch.nn.functional as F\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "from diffusers import DDPMPipeline\n",
    "import math\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    '''\n",
    "    Class for training parameters\n",
    "    '''\n",
    "    \n",
    "    image_size = 128  # the generated image resolution\n",
    "    train_batch_size = 8\n",
    "    eval_batch_size = 8  # how many images to sample during evaluation\n",
    "    num_epochs = 200\n",
    "    gradient_accumulation_steps = 1\n",
    "    learning_rate = 1e-4\n",
    "    lr_warmup_steps = 500\n",
    "    save_image_epochs = 10\n",
    "    save_model_epochs = 30\n",
    "    mixed_precision = \"fp16\"  # `no` for float32, `fp16` for automatic mixed precision\n",
    "    output_dir = \"ddpm-butterflies-128\"  # the model name locally and on the HF Hub\n",
    "\n",
    "    push_to_hub = True  # whether to upload the saved model to the HF Hub\n",
    "    hub_private_repo = False\n",
    "    overwrite_output_dir = True  # overwrite the old model when re-running the notebook\n",
    "    seed = 0\n",
    "\n",
    "\n",
    "config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pickle files using the exact file paths\n",
    "x_data_path = \"/home/maddie/Documents/underwater/DeepCFD/dataX.pkl\"\n",
    "y_data_path = \"/home/maddie/Documents/underwater/DeepCFD/dataY.pkl\"\n",
    "\n",
    "with open(x_data_path, \"rb\") as f:\n",
    "    x = pickle.load(f)\n",
    "\n",
    "with open(y_data_path, \"rb\") as f:\n",
    "    y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the matplotlib figure and axes\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))  # 1 row, 3 columns for the 3 channels\n",
    "\n",
    "# temp = y[12,0]\n",
    "# temp = np.where(temp<=0, 0, temp)\n",
    "# y[12,0] = temp\n",
    "\n",
    "# Plot each channel\n",
    "for i in range(3):  # Loop over the channels\n",
    "    ax = axs[i]  # Get the current axis\n",
    "    channel_data = y[12, i, :, :] # Get the data for the first sample, channel i\n",
    "    im = ax.imshow(channel_data, cmap='jet', aspect='auto')  # Plot the data\n",
    "    ax.set_title(f'Channel {i+1}')  # Set the title for the channel\n",
    "    fig.colorbar(im, ax=ax)  # Add a colorbar for each plot\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(y[12,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize input data\n",
    "# Assuming you want to randomly sample an input data point\n",
    "index = 301\n",
    "\n",
    "# Plotting only the input data (input features)\n",
    "input_data = x[index]\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(input_data.transpose(1, 2, 0), cmap = \"jet\")  # Transpose dimensions for correct display\n",
    "plt.title('Input Data')\n",
    "plt.xlabel('Width')\n",
    "plt.ylabel('Height')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the first 50 inputs in a 5x10 grid subplot\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "for i in range(50):\n",
    "    plt.subplot(5, 10, i + 1)\n",
    "    input_data = x[i]\n",
    "    plt.imshow(input_data.transpose(1, 2, 0))  # Transpose dimensions for correct display\n",
    "    plt.title('Input {}'.format(i+1))\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape\n",
    "x[:,2].min()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "\n",
    "# turn the input in a pytorch tensor\n",
    "x = torch.FloatTensor(x)\n",
    "y = torch.FloatTensor(y)\n",
    "\n",
    "# Shuffle data \n",
    "indices = list(range(len(x)))\n",
    "np.random.shuffle(indices)\n",
    "x = x[indices]\n",
    "y = y[indices]\n",
    "\n",
    "# Normalize the data [-1,1] for each channel \n",
    "def normalize_tensor(tensor):\n",
    "    min_val = torch.amin(tensor, dim=(0, 2, 3), keepdim=True)  # min over all axes except channels\n",
    "    max_val = torch.amax(tensor, dim=(0, 2, 3), keepdim=True)  # max over all axes except channels\n",
    "    # Normalize to [-1, 1]\n",
    "    tensor_normalized = 2 * ((tensor - min_val) / (max_val - min_val)) - 1\n",
    "    return tensor_normalized\n",
    "\n",
    "# normalize the input and output data\n",
    "x = normalize_tensor(x)\n",
    "y = normalize_tensor(y)\n",
    "\n",
    "def test_normalization(normalize_tensor):\n",
    "\n",
    "    # Iterate over each channel to check and print min/max values\n",
    "    for c in range(normalize_tensor.size(1)):  # tensor.size(1) gives the number of channels\n",
    "        channel_normalized = normalize_tensor[:, c, :, :]\n",
    "        \n",
    "        # Calculate min and max values for the normalized channel\n",
    "        min_val = torch.amin(channel_normalized).item()\n",
    "        max_val = torch.amax(channel_normalized).item()\n",
    "        \n",
    "        # Print the min and max values\n",
    "        print(f'Channel {c+1}: Min = {min_val}, Max = {max_val}')\n",
    "        \n",
    "        # Check that min and max values are -1 and 1, respectively\n",
    "        assert min_val >= -1 and max_val <= 1, f\"Channel {c+1} is not properly normalized.\"\n",
    "    \n",
    "    print(\"All checks passed. The tensor is normalized correctly.\")\n",
    "\n",
    "# Assuming normalize_tensor is defined as before, you can now test this function:\n",
    "print(\"Check if input is normalized: \")\n",
    "test_normalization(x)\n",
    "print(\"\\nCheck if output is normalized: \")\n",
    "test_normalization(y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))  # 1 row, 3 columns for the 3 channels\n",
    "# Plot each channel\n",
    "for i in range(3):  # Loop over the channels\n",
    "    ax = axs[i]  # Get the current axis\n",
    "    channel_data = y[12, i, :, :] # Get the data for the first sample, channel i\n",
    "    im = ax.imshow(channel_data, cmap='jet', aspect='auto')  # Plot the data\n",
    "    ax.set_title(f'Channel {i+1}')  # Set the title for the channel\n",
    "    fig.colorbar(im, ax=ax)  # Add a colorbar for each plot\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function for dividing the dataset \n",
    "def split_tensors(*tensors, ratio):\n",
    "    preprocess = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((128,128)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            # transforms.ToTensor(),\n",
    "            # transforms.Normalize([0.5], [0.5]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    assert len(tensors) > 0\n",
    "    split1, split2 = [], []\n",
    "    count = len(tensors[0])\n",
    "    for tensor in tensors:\n",
    "        assert len(tensor) == count\n",
    "        # Interpreting the last two dimensions as 128\n",
    "        tensor = [preprocess(temp) for temp in tensor]\n",
    "        tensor = torch.stack(tensor)\n",
    "        split1.append(tensor[:int(len(tensor) * ratio)])\n",
    "        split2.append(tensor[int(len(tensor) * ratio):])\n",
    "    if len(tensors) == 1:\n",
    "        split1, split2 = split1[0], split2[0]\n",
    "    return split1, split2\n",
    "\n",
    "# # Split the data into training and testing sets (70/30)\n",
    "# train_data, test_data = split_tensors(x,y, ratio=0.7)\n",
    "train_data, test_data = split_tensors(y,ratio=0.7)\n",
    "\n",
    "# Create PyTorch datasets \n",
    "train_dataset = TensorDataset(train_data)\n",
    "\n",
    "test_dataset = TensorDataset(test_data)\n",
    "# test_x, test_y = test_data[:] # split test data into x and y\n",
    "\n",
    "# Load data into PyTorch DataLoader\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=config.train_batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Scheduler \n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000) # DDPM scheduler with 1000 timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the effect of noise on an image\n",
    "def visualize_noisy_image(image_tensor, noise_scheduler, timesteps):\n",
    "    # Generate random noise\n",
    "    noise = torch.randn(image_tensor.shape)\n",
    "\n",
    "    # Add noise using the scheduler\n",
    "    noisy_image = noise_scheduler.add_noise(image_tensor, noise, timesteps)\n",
    "\n",
    "    # Convert the noisy image for visualization\n",
    "    noisy_image_to_display = Image.fromarray(((noisy_image.permute(0, 2, 3, 1).detach() + 1) * 127.5).type(torch.uint8).numpy()[0])\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(noisy_image_to_display)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize a noisy version of the first sample in the training data\n",
    "# You can change the timestep to see the effect of different noise levels\n",
    "visualize_noisy_image(y[0:1], noise_scheduler, timesteps=torch.LongTensor([50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize adding noise at different timesteps\n",
    "def visualize_noisy_image(image_tensor, noise_scheduler, timesteps):\n",
    "    fig, axes = plt.subplots(1, len(timesteps), figsize=(20, 4))\n",
    "    \n",
    "    for i, step in enumerate(timesteps):\n",
    "        # Generate random noise\n",
    "        noise = torch.randn_like(image_tensor)\n",
    "        # Add noise using the scheduler\n",
    "        noisy_image = noise_scheduler.add_noise(image_tensor, noise, step)\n",
    "\n",
    "        # Normalize the noisy image for visualization\n",
    "        noisy_image_to_display = ((noisy_image.squeeze().detach() + 1) * 127.5).clamp(0, 255).type(torch.uint8)\n",
    "        noisy_image_to_display = noisy_image_to_display.permute(1, 2, 0).numpy()\n",
    "\n",
    "        # Display the image\n",
    "        axes[i].imshow(noisy_image_to_display)\n",
    "        axes[i].set_title(f'Timestep {step.item()}')\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Visualize a noisy version of the first sample in the training data at different timesteps\n",
    "timesteps = torch.LongTensor([0, 100, 200, 300, 400, 500, 600, 700]) # example timesteps\n",
    "visualize_noisy_image(y[0:1], noise_scheduler, timesteps=timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup NN model\n",
    "model = UNet2DModel(\n",
    "    sample_size=config.image_size,  # the target image resolution\n",
    "    in_channels=3,  # the number of input channels, 3 for RGB images\n",
    "    out_channels=3,  # the number of output channels\n",
    "    layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
    "    block_out_channels=(128, 128, 256, 256, 512, 512),  # the number of output channels for each UNet block\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",  # a regular ResNet downsampling block\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
    "        \"DownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\",  # a regular ResNet upsampling block\n",
    "        \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = UNet2DConditionalModel( \n",
    "#     sample_size: Optional = None\n",
    "#     in_channels: int = 4\n",
    "#     out_channels: int = 4\n",
    "#     center_input_sample:  bool = False\n",
    "#     flip_sin_to_cos: bool = Tru\n",
    "#     freq_shift: int = 0\n",
    "#     down_block_types: Tuple = ('CrossAttnDownBlock2D', 'CrossAttnDownBlock2D', 'CrossAttnDownBlock2D', 'DownBlock2D'\n",
    "#     mid_block_type: Optional = 'UNetMidBlock2DCrossAttn'\n",
    "#     up_block_types:  Tuple = ('UpBlock2D', 'CrossAttnUpBlock2D', 'CrossAttnUpBlock2D', 'CrossAttnUpBlock2D')\n",
    "#     only_cross_attention:  Union = False\n",
    "#     block_out_channels: Tuple = (320, 640, 1280, 1280)\n",
    "#     layers_per_block:  Union = 2\n",
    "#     downsample_padding: int = 1\n",
    "#     mid_block_scale_factor: float = 1\n",
    "#     dropout: float = 0.0\n",
    "#     act_fn: str = 'silu'\n",
    "#     norm_num_groups: Optional = 32\n",
    "#     norm_eps: float = 1e-05\n",
    "#     cross_attention_dim: Union = 1280\n",
    "#     transformer_layers_per_block: Union = 1\n",
    "#     reverse_transformer_layers_per_block: Optional = None\n",
    "#     econder_hid_dim: Optional = None\n",
    "#     encoder_hid_dim_type: Optional = None\n",
    "#     attention_head_dim: Union = 8\n",
    "#     num_attention_heads: Union = None\n",
    "#     dual_cross_attention: bool = False\n",
    "#     use_linear_projection: bool = False\n",
    "#     class_embed_type: Optional = None\n",
    "#     addition_embed_type: Optional = None\n",
    "#     addition_time_embed_dim: Optional = None\n",
    "#     num_class_embeds:  Optional = None\n",
    "#     upcast_attention: bool = False\n",
    "#     resnet_time_scale_shift: str = 'default'\n",
    "#     resnet_skip_time_act: bool = False\n",
    "#     resent_out_scale_factor: float = 1.0\n",
    "#     time_embedding_type: str = 'positional'\n",
    "#     time_embedding_dim: Optional = None\n",
    "#     time_embedding_act_fn: Optional = None\n",
    "#     timestep_post_act: Optional = None\n",
    "#     time_cond_proj_dim: Optional = None\n",
    "#     conv_in_kernel: int = 3\n",
    "#     conv_out_kernel: int = 3\n",
    "#     projection_class_embddings_input_dim: Optional = None\n",
    "#     attention_type: str = 'default'\n",
    "#     class_embeddings_concat:  bool = False\n",
    "#     mid_block_only_cross_attention: Optional = None\n",
    "#     cross_attention_norm: Optional = None\n",
    "#     addition_embed_type_num_heads: int = 64\n",
    "\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer:  update the model's parameters based on the computed gradients during the training process\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "# Learning rate scheduler: adjust the learning rate of the optimizer during the training process\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=config.lr_warmup_steps,\n",
    "    num_training_steps=(len(train_dataloader) * config.num_epochs),\n",
    ")\n",
    "\n",
    "# Function for evaluation?\n",
    "def evaluate(config, epoch, model, noise_scheduler, val_dataloader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    total_mse = 0\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for batch in val_dataloader:\n",
    "            clean_images = batch[0].to(device)\n",
    "            noise = torch.randn(clean_images.shape).to(device)\n",
    "            bs = clean_images.shape[0]\n",
    "            timesteps = torch.randint(\n",
    "                0, noise_scheduler.config.num_train_timesteps, (bs,), device=device\n",
    "            ).long()\n",
    "            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "            noise_pred = model(noisy_images, timesteps)[0]\n",
    "            mse = F.mse_loss(noise_pred, noise, reduction='sum').item()  # Calculate batch MSE\n",
    "            total_mse += mse\n",
    "\n",
    "    avg_mse = total_mse / len(val_dataloader.dataset)\n",
    "    print(f\"Epoch {epoch}: Average MSE = {avg_mse}\")\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Method to evaluate the model\n",
    "\n",
    "# Define the training loop function\n",
    "def train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler):\n",
    "    \n",
    "    # Check if an output directory is specified and create it if it doesn't exist\n",
    "    if config.output_dir is not None:\n",
    "        os.makedirs(config.output_dir, exist_ok=True)\n",
    "        # Initialize any desired loggers here, such as TensorBoard\n",
    "\n",
    "    # Initialize a global step counter\n",
    "    global_step = 0\n",
    "\n",
    "    # GPU/CPU setup\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Start the training process\n",
    "    for epoch in range(config.num_epochs):\n",
    "        \n",
    "        # Initialize a progress bar for the epoch\n",
    "        progress_bar = tqdm(total=len(train_dataloader), desc=f\"Epoch {epoch}\")\n",
    "        \n",
    "        # Iterate over the training data loader\n",
    "        for _, batch in enumerate(train_dataloader):\n",
    "\n",
    "            # Move the images from the current batch to the device\n",
    "            clean_images = batch[0].to(device)\n",
    "\n",
    "            # Generate random noise of the same shape as the images\n",
    "            noise = torch.randn(clean_images.shape).to(device)\n",
    "\n",
    "            # Get the batch size from the images shape\n",
    "            bs = clean_images.shape[0]\n",
    "\n",
    "            # Sample random timesteps for each image in the batch\n",
    "            timesteps = torch.randint(\n",
    "                0, noise_scheduler.config.num_train_timesteps, (bs,), device=device\n",
    "            ).long()\n",
    "\n",
    "            # Add noise to the clean images according to the noise magnitude at the sampled timesteps\n",
    "            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "\n",
    "            # print(\"noisy data:\", noisy_images.shape)\n",
    "            # print(\"timesteps:\", timesteps.shape)\n",
    "\n",
    "            # Predict the noise residual using the model\n",
    "            noise_pred = model(noisy_images, timesteps).sample\n",
    "            # print(\"noise_pred:\", noise_pred.shape)\n",
    "            \n",
    "            # Calculate the mean squared error loss between the predicted noise and the actual noise\n",
    "            loss = F.mse_loss(noise_pred, noise)\n",
    "\n",
    "            # Perform backpropagation: zero the gradients, calculate gradients, and perform a single optimization step\n",
    "            optimizer.zero_grad()  # Clear previous gradients\n",
    "            loss.backward()  # Calculate gradients through backpropagation\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Clip gradients to avoid exploding gradient problem\n",
    "            optimizer.step()  # Update model parameters\n",
    "            lr_scheduler.step()  # Update learning rate scheduler\n",
    "\n",
    "            # Update the progress bar and log the loss and learning rate\n",
    "            progress_bar.update(1)\n",
    "            logs = {\"loss\": loss.item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            # Log your progress here, replacing 'accelerator.log' with your logger\n",
    "            global_step += 1  # Increment the global step counter\n",
    "\n",
    "        # After each epoch, evaluate your model and save it if necessary\n",
    "        if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "            # Replace 'evaluate' with your evaluation function\n",
    "            evaluate(config, epoch, model, noise_scheduler, train_dataloader)\n",
    "\n",
    "        if (epoch + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "            # Save your model parameters to the specified output directory\n",
    "            torch.save(model.state_dict(), os.path.join(config.output_dir, f\"model_epoch_{epoch}.pth\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model \n",
    "train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = next(train_dataloader.__iter__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for temp in train_dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cfd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
